{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814a16a1-c98c-45e6-9621-8eb31ba7d12d",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fee58ee-fbd6-4157-a814-2da3ffc6853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from load_data.prepared_custom_ds import CustomDataset\n",
    "from utilities.config_load import load_config\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f6ff6b-229e-4478-9862-2a991506dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f5011-507a-4dd1-bf32-6cbff77d4394",
   "metadata": {},
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79161ad9-479e-40b8-9473-af4378778422",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6dbbe-1417-4ab0-9eee-8e1abc92c0a0",
   "metadata": {},
   "source": [
    "# Load data and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37dfc710-3e9e-4245-8767-4e5a1399b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/\"\n",
    "config = load_config(CONFIG_PATH, \"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4b677b-49d8-4f45-82ed-033d487aad98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'train_img_path': 'dataset/train_v2',\n",
       "  'test_img_path': 'dataset/test_v2',\n",
       "  'mask_path': 'dataset/train_masks',\n",
       "  'reshaped_img_path': 'dataset/reshaped_img',\n",
       "  'dir_path': 'dataset'},\n",
       " 'original_img_size': 768,\n",
       " 'new_img_size': 256,\n",
       " 'project_path': 'C:/Users/da4nik/Segmentation',\n",
       " 'model': {'svs_path': 'model_svs/'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4b231d-8ed5-4db2-9fca-fc7ad0cf896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(pd.read_csv(osp.join(config['dataset']['dir_path'], \n",
    "                                                'train_ship_segmentations_v2.csv'))[\"EncodedPixels\"].fillna('').str.split())\n",
    "img_ids = list(pd.read_csv(osp.join(config['dataset']['dir_path'], \n",
    "                                                'train_ship_segmentations_v2.csv'))[\"ImageId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f47e9e-8bef-4fa9-962f-ad031aa63c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_ids, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccd643-cec3-49d5-bbc5-10bee5f5ce91",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3ac38d-367f-4924-a94a-ecc8e828b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(first_chanels, second_chanels, kernel_size, dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(first_chanels),\n",
    "        nn.Conv2d(first_chanels, second_chanels, kernel_size, padding='same'),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.BatchNorm2d(second_chanels),\n",
    "        nn.Conv2d(second_chanels, second_chanels, kernel_size, padding='same'),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc2235b-57a9-4deb-a622-32b084af51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_Encoder(nn.Module):\n",
    "    def __init__(self, kernel_size, dropout_rate, nkernels):\n",
    "        super(Unet_Encoder, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nkernels = nkernels\n",
    "        self.conv1 = ConvBlock(3, nkernels, self.kernel_size, self.dropout_rate)\n",
    "        self.conv2 = ConvBlock(nkernels, nkernels*2, self.kernel_size, self.dropout_rate)\n",
    "        self.conv3 = ConvBlock(nkernels*2, nkernels*4, self.kernel_size, self.dropout_rate)\n",
    "        self.conv4 = ConvBlock(nkernels*4, nkernels*8, self.kernel_size, self.dropout_rate)\n",
    "        self.maxpool_list = nn.ModuleList([nn.MaxPool2d(kernel_size=2) for _ in range(4)])\n",
    "        self.conv_list = nn.ModuleList([self.conv1, self.conv2, self.conv3, self.conv4])\n",
    "\n",
    "    '''def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.01)'''\n",
    "\n",
    "    def forward(self, input):\n",
    "        list_skips = list()\n",
    "        for i in range(4):\n",
    "            skip = self.conv_list[i](input)\n",
    "            input = self.maxpool_list[i](skip)\n",
    "            list_skips.append(skip)\n",
    "        return input, list_skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b17548d-5952-44b6-9c80-197f5e78ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_Decoder(nn.Module):\n",
    "    def __init__(self, kernel_size, dropout_rate, nkernels):\n",
    "        super(Unet_Decoder, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nkernels = nkernels\n",
    "        self.conv5 = ConvBlock(nkernels*8, nkernels*16, self.kernel_size, self.dropout_rate)\n",
    "        self.conv6 = ConvBlock(nkernels*16, nkernels*8, self.kernel_size, self.dropout_rate)\n",
    "        self.conv7 = ConvBlock(nkernels*8, nkernels*4, self.kernel_size, self.dropout_rate)\n",
    "        self.conv8 = ConvBlock(nkernels*4, nkernels*2, self.kernel_size, self.dropout_rate)\n",
    "        self.conv_list = nn.ModuleList([self.conv5, self.conv6, self.conv7, self.conv8])\n",
    "        self.convt_list = nn.ModuleList([nn.ConvTranspose2d(nkernels*(2**(4-i)), nkernels*((2**(4-i))//2), kernel_size=(2, 2), stride=(2, 2)) \n",
    "                                           for i in range(4)])\n",
    "\n",
    "    \n",
    "    '''def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.01)'''\n",
    "\n",
    "    \n",
    "    def forward(self, input, list_skips):\n",
    "        for i in range(4):\n",
    "            if i==0:\n",
    "                out = self.conv_list[i](input)\n",
    "                out = self.convt_list[i](out)\n",
    "            else:\n",
    "                out = self.conv_list[i](torch.cat((out, list_skips[4-i]), 1)) # channel\n",
    "                out = self.convt_list[i](out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549f3734-7c7d-477c-8f1b-02a2729af086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Unet(nn.Module):\n",
    "    def __init__(self, kernel_size, dropout_rate, nkernels, output_chanels):\n",
    "        super(Model_Unet, self).__init__()\n",
    "        self.output_chanels = output_chanels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nkernels = nkernels\n",
    "        self.enc_layer = Unet_Encoder(self.kernel_size, self.dropout_rate, self.nkernels)\n",
    "        self.dec_layer = Unet_Decoder(self.kernel_size, self.dropout_rate, self.nkernels)\n",
    "        self.conv9 = ConvBlock(self.nkernels*2, self.nkernels, self.kernel_size, self.dropout_rate)\n",
    "        self.conv10 = nn.Conv2d(self.nkernels, self.output_chanels, (1, 1), padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    '''def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.01)'''\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out, list_skips = self.enc_layer(input)\n",
    "        out = self.dec_layer(out, list_skips)\n",
    "        out = self.conv9(torch.cat((out, list_skips[0]), 1)) # channel concat\n",
    "        out = self.relu(out)\n",
    "        out = self.conv10(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a0b06-2a57-4eee-9a44-1347b6c4f8fe",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "823c5adc-4d0c-4ced-9c28-e891a8326d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, opt, loader):\n",
    "    loss_per_batches = 0\n",
    "    elapsed = 0\n",
    "    start_epoch2 = time.time()\n",
    "    for i, data in tqdm(enumerate(loader), total=231000//160):\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        y_pred = model(features)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        loss_per_batches += loss\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        elapsed += (end_epoch - start_epoch)\n",
    "        \n",
    "    print(\"train = \" + str(elapsed))\n",
    "    print(\"train + load = \" + str(time.time() - start_epoch2))\n",
    "    return loss_per_batches/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7281747-1f6d-4bf2-9201-077f8fdfacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, train_loader, val_loader, save_treshold=25, epochs=50, model_name='model_name'):\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = train_step(model, loss_fn, opt, train_loader)\n",
    "        model.eval()\n",
    "\n",
    "        vloss = 0\n",
    "        counter = 0\n",
    "        with torch.inference_mode():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeatures, vlabels = vdata\n",
    "                vfeatures, vlabels = vfeatures.to(device), vlabels.to(device)\n",
    "\n",
    "                y_pred = model(vfeatures)\n",
    "                vloss += loss_fn(y_pred, vlabels)\n",
    "                counter = i\n",
    "\n",
    "        avg_vloss = vloss / (counter + 1)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        \n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % save_treshold == 0:\n",
    "            model_path = config['model']['svs_path'] + model_name +'_{}_{}'.format(timestamp, (epoch + 1))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        print(\"Time per epoch {}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cace7-86e9-4658-8681-1aecba0170dc",
   "metadata": {},
   "source": [
    "# Creating dataloader and model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa92cd3-f627-49bb-a439-437f700c3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(config, X_train, y_train)\n",
    "vdataset = CustomDataset(config, X_test, y_test)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=160, num_workers=0, shuffle=False)\n",
    "vdataloader = torch.utils.data.DataLoader(vdataset, batch_size=160, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b3c46c-e6d1-4604-8952-ae0fc197ecdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model_Unet(kernel_size=3, dropout_rate=0.15, nkernels=2, output_chanels=1)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c878a3b1-3e5e-4064-a2e6-ef08856f2609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Unet_Encoder: 1-1                      --\n",
      "|    └─Sequential: 2-1                   --\n",
      "|    |    └─BatchNorm2d: 3-1             6\n",
      "|    |    └─Conv2d: 3-2                  56\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    |    └─Dropout: 3-4                 --\n",
      "|    |    └─BatchNorm2d: 3-5             4\n",
      "|    |    └─Conv2d: 3-6                  38\n",
      "|    |    └─ReLU: 3-7                    --\n",
      "|    └─Sequential: 2-2                   --\n",
      "|    |    └─BatchNorm2d: 3-8             4\n",
      "|    |    └─Conv2d: 3-9                  76\n",
      "|    |    └─ReLU: 3-10                   --\n",
      "|    |    └─Dropout: 3-11                --\n",
      "|    |    └─BatchNorm2d: 3-12            8\n",
      "|    |    └─Conv2d: 3-13                 148\n",
      "|    |    └─ReLU: 3-14                   --\n",
      "|    └─Sequential: 2-3                   --\n",
      "|    |    └─BatchNorm2d: 3-15            8\n",
      "|    |    └─Conv2d: 3-16                 296\n",
      "|    |    └─ReLU: 3-17                   --\n",
      "|    |    └─Dropout: 3-18                --\n",
      "|    |    └─BatchNorm2d: 3-19            16\n",
      "|    |    └─Conv2d: 3-20                 584\n",
      "|    |    └─ReLU: 3-21                   --\n",
      "|    └─Sequential: 2-4                   --\n",
      "|    |    └─BatchNorm2d: 3-22            16\n",
      "|    |    └─Conv2d: 3-23                 1,168\n",
      "|    |    └─ReLU: 3-24                   --\n",
      "|    |    └─Dropout: 3-25                --\n",
      "|    |    └─BatchNorm2d: 3-26            32\n",
      "|    |    └─Conv2d: 3-27                 2,320\n",
      "|    |    └─ReLU: 3-28                   --\n",
      "|    └─ModuleList: 2-5                   --\n",
      "|    |    └─MaxPool2d: 3-29              --\n",
      "|    |    └─MaxPool2d: 3-30              --\n",
      "|    |    └─MaxPool2d: 3-31              --\n",
      "|    |    └─MaxPool2d: 3-32              --\n",
      "|    └─ModuleList: 2-6                   --\n",
      "|    |    └─Sequential: 3-33             (recursive)\n",
      "|    |    └─Sequential: 3-34             (recursive)\n",
      "|    |    └─Sequential: 3-35             (recursive)\n",
      "|    |    └─Sequential: 3-36             (recursive)\n",
      "├─Unet_Decoder: 1-2                      --\n",
      "|    └─Sequential: 2-7                   --\n",
      "|    |    └─BatchNorm2d: 3-37            32\n",
      "|    |    └─Conv2d: 3-38                 4,640\n",
      "|    |    └─ReLU: 3-39                   --\n",
      "|    |    └─Dropout: 3-40                --\n",
      "|    |    └─BatchNorm2d: 3-41            64\n",
      "|    |    └─Conv2d: 3-42                 9,248\n",
      "|    |    └─ReLU: 3-43                   --\n",
      "|    └─Sequential: 2-8                   --\n",
      "|    |    └─BatchNorm2d: 3-44            64\n",
      "|    |    └─Conv2d: 3-45                 4,624\n",
      "|    |    └─ReLU: 3-46                   --\n",
      "|    |    └─Dropout: 3-47                --\n",
      "|    |    └─BatchNorm2d: 3-48            32\n",
      "|    |    └─Conv2d: 3-49                 2,320\n",
      "|    |    └─ReLU: 3-50                   --\n",
      "|    └─Sequential: 2-9                   --\n",
      "|    |    └─BatchNorm2d: 3-51            32\n",
      "|    |    └─Conv2d: 3-52                 1,160\n",
      "|    |    └─ReLU: 3-53                   --\n",
      "|    |    └─Dropout: 3-54                --\n",
      "|    |    └─BatchNorm2d: 3-55            16\n",
      "|    |    └─Conv2d: 3-56                 584\n",
      "|    |    └─ReLU: 3-57                   --\n",
      "|    └─Sequential: 2-10                  --\n",
      "|    |    └─BatchNorm2d: 3-58            16\n",
      "|    |    └─Conv2d: 3-59                 292\n",
      "|    |    └─ReLU: 3-60                   --\n",
      "|    |    └─Dropout: 3-61                --\n",
      "|    |    └─BatchNorm2d: 3-62            8\n",
      "|    |    └─Conv2d: 3-63                 148\n",
      "|    |    └─ReLU: 3-64                   --\n",
      "|    └─ModuleList: 2-11                  --\n",
      "|    |    └─Sequential: 3-65             (recursive)\n",
      "|    |    └─Sequential: 3-66             (recursive)\n",
      "|    |    └─Sequential: 3-67             (recursive)\n",
      "|    |    └─Sequential: 3-68             (recursive)\n",
      "|    └─ModuleList: 2-12                  --\n",
      "|    |    └─ConvTranspose2d: 3-69        2,064\n",
      "|    |    └─ConvTranspose2d: 3-70        520\n",
      "|    |    └─ConvTranspose2d: 3-71        132\n",
      "|    |    └─ConvTranspose2d: 3-72        34\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─BatchNorm2d: 2-13                 8\n",
      "|    └─Conv2d: 2-14                      74\n",
      "|    └─ReLU: 2-15                        --\n",
      "|    └─Dropout: 2-16                     --\n",
      "|    └─BatchNorm2d: 2-17                 4\n",
      "|    └─Conv2d: 2-18                      38\n",
      "|    └─ReLU: 2-19                        --\n",
      "├─Conv2d: 1-4                            3\n",
      "├─ReLU: 1-5                              --\n",
      "├─Sigmoid: 1-6                           --\n",
      "=================================================================\n",
      "Total params: 30,937\n",
      "Trainable params: 30,937\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "summary(model)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f6e2d-70fd-40ac-94df-0b7d8d896dc5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702706e3-db9d-43b8-8430-b6f6aedc2696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1443 [00:00<?, ?it/s]C:\\Users\\da4nik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      " 90%|██████████████████████████████████████████████████████████████████████▍       | 1304/1443 [13:46<01:28,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 147.50356078147888\n",
      "train + load = 826.4572033882141\n",
      "LOSS train 0.0908844918012619 valid 0.005149719305336475\n",
      "Time per epoch 910.5090363025665s\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████▍       | 1304/1443 [13:33<01:26,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 108.46459126472473\n",
      "train + load = 813.9887747764587\n",
      "LOSS train 0.004183068871498108 valid 0.004187147133052349\n",
      "Time per epoch 897.270733833313s\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████▍       | 1304/1443 [13:43<01:27,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 128.67351508140564\n",
      "train + load = 823.1553995609283\n",
      "LOSS train 0.003634538734331727 valid 0.0035843043588101864\n",
      "Time per epoch 905.8035776615143s\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                        | 115/1443 [01:13<14:05,  1.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst_try\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, opt, train_loader, val_loader, save_treshold, epochs, model_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 12\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     15\u001b[0m vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, loss_fn, opt, loader)\u001b[0m\n\u001b[0;32m      3\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m start_epoch2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m231000\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m160\u001b[39m):\n\u001b[0;32m      7\u001b[0m     start_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m     features, labels \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Segmentation\\load_data\\prepared_custom_ds.py:40\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m lbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     39\u001b[0m mask \u001b[38;5;241m=\u001b[39m Create_mask(lbl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size)\n\u001b[1;32m---> 40\u001b[0m res_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m img_id)\n\u001b[0;32m     42\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mtranspose(image, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:492\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:462\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias \u001b[38;5;129;01mand\u001b[39;00m interpolation \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# We manually set it to False to avoid an error downstream in interpolate()\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# This behaviour is documented: the parameter is irrelevant for modes\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# that are not bilinear or bicubic. We used to raise an error here, but\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# now we don't as True is the default.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     antialias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m img, need_cast, need_squeeze, out_dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_cast_squeeze_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[0;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:528\u001b[0m, in \u001b[0;36m_cast_squeeze_in\u001b[1;34m(img, req_dtypes)\u001b[0m\n\u001b[0;32m    526\u001b[0m     need_cast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     req_dtype \u001b[38;5;241m=\u001b[39m req_dtypes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 528\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, need_cast, need_squeeze, out_dtype\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, dataloader, vdataloader, 50, epochs=100, model_name='first_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e895712-6939-4584-a870-41117ffa03d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb699f5-e631-41fb-ad5d-9ca9f855cd94",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5027d8-c678-4f60-ba8b-a9d1f147e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 17380/17380 [06:27<00:00, 44.88it/s]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for data in tqdm(dataloader):\n",
    "    features, labels = data\n",
    "    try:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "    except:\n",
    "        print(features)\n",
    "        print(str(features.dtype) + \" -> \" + str(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea8cb0-511a-4c89-a543-1426f93dc9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(torch.zeros(16, 3, 256, 256).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a110e255-6a3b-4d3b-86e5-6c253fd17b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(16, 3, 256, 256).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937e542-b010-4098-ad67-fc3fb8add3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50e83f-ba7a-4047-b144-b0820d984e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = torch.zeros(16, 1, 200, 200).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847b39e-f150-4e5a-8f8a-75ff36abce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb69ff-049a-4abc-b929-d0057365af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f38f8f-9607-4f1e-b936-2c29fd5fdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = torch.ones(16, 1, 200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b96238e-6f56-49ff-ad40-dbb337aeec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e90d4f8a-afb6-46a5-8219-efba9671f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = loss_fun(torch.ones(16, 1, 200, 200), torch.rand(16, 1, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c2dd7f-0e3b-41e1-bc89-a43b69e94032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49.9320)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53af7dd-f56b-4351-9c69-84eb2311bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(str(2**(4-i)) + \" => \" + str((2**(4-i))//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c20bae-5eb1-46cc-93c4-2d04caa62cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = nn.MaxPool2d(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4567af-bc8d-4ddb-ae6f-bc3bac28195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = nn.ConvTranspose2d(16, 16//2, kernel_size=(2, 2), stride=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ec1f0-fcfc-4853-93b3-67c96afbfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = max(torch.zeros(16, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc500487-ea3f-49fc-9776-21d2e8ed34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fec0b9-c395-476f-8752-d8a9af8cf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ct(torch.zeros(16, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e94b00-06ba-4514-969b-1d0369b549f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e117d-59d3-45dc-8939-97c190fb0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ecc16-f5e7-4309-b830-afc25a73f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, i in enumerate(dataloader):\n",
    "    print(i[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810c414-81ac-48b8-9b2a-b6e02b339945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
